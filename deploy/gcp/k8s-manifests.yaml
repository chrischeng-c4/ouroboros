# Kubernetes manifests for data-bridge PyLoop with OpenTelemetry on GKE
#
# This deploys the application with an OpenTelemetry Collector sidecar
# that forwards traces to Google Cloud Trace.
#
# Architecture:
# - Application container: data-bridge-pyloop app
# - Sidecar container: OpenTelemetry Collector
# - Both containers share the same pod network (localhost)
# - Application sends traces to localhost:4317 (OTel Collector)
# - Collector forwards to GCP Cloud Trace
#
# Prerequisites:
# - GKE cluster with Workload Identity enabled
# - Google Cloud Trace API enabled
# - Service account with Cloud Trace Agent role

---
# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: data-bridge
  labels:
    app: data-bridge
    environment: production

---
# ConfigMap for OpenTelemetry Collector configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: data-bridge
  labels:
    app: data-bridge
    component: otel-collector
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      batch:
        timeout: 5s
        send_batch_size: 512
        send_batch_max_size: 1024

      resourcedetection:
        detectors:
          - env
          - gcp
        timeout: 5s

      memory_limiter:
        check_interval: 1s
        limit_mib: 512
        spike_limit_mib: 128

      resource:
        attributes:
          - key: service.namespace
            value: data-bridge
            action: insert

    exporters:
      googlecloud:
        trace:
          compression: gzip
        timeout: 12s

    extensions:
      health_check:
        endpoint: 0.0.0.0:13133

    service:
      extensions:
        - health_check
      telemetry:
        logs:
          level: info
        metrics:
          level: detailed
          address: 0.0.0.0:8888
      pipelines:
        traces:
          receivers:
            - otlp
          processors:
            - memory_limiter
            - resourcedetection
            - resource
            - batch
          exporters:
            - googlecloud

---
# Service Account with Workload Identity
apiVersion: v1
kind: ServiceAccount
metadata:
  name: data-bridge-app
  namespace: data-bridge
  annotations:
    # Workload Identity binding
    # Replace with your GCP service account
    iam.gke.io/gcp-service-account: data-bridge@PROJECT_ID.iam.gserviceaccount.com
  labels:
    app: data-bridge

---
# Deployment with Sidecar pattern
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-bridge-api
  namespace: data-bridge
  labels:
    app: data-bridge
    component: api
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: data-bridge
      component: api
  template:
    metadata:
      labels:
        app: data-bridge
        component: api
        version: v1
      annotations:
        # Prometheus scraping annotations
        prometheus.io/scrape: "true"
        prometheus.io/port: "8888"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: data-bridge-app

      # Init container to wait for dependencies (optional)
      # initContainers:
      #   - name: wait-for-mongodb
      #     image: busybox:1.36
      #     command: ['sh', '-c', 'until nc -z mongodb 27017; do echo waiting for mongodb; sleep 2; done']

      containers:
        # Application container
        - name: app
          image: gcr.io/PROJECT_ID/data-bridge-api:latest
          imagePullPolicy: Always

          ports:
            - name: http
              containerPort: 8000
              protocol: TCP

          env:
            # Application configuration
            - name: APP_HOST
              value: "0.0.0.0"
            - name: APP_PORT
              value: "8000"
            - name: DEPLOYMENT_ENV
              value: "production"

            # MongoDB configuration
            - name: MONGODB_URI
              valueFrom:
                secretKeyRef:
                  name: data-bridge-secrets
                  key: mongodb-uri

            # OpenTelemetry configuration
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "http://localhost:4317"
            - name: OTEL_SERVICE_NAME
              value: "data-bridge-api"
            - name: OTEL_SERVICE_VERSION
              value: "0.1.0"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: "service.namespace=data-bridge,deployment.environment=production"

            # GCP configuration (auto-detected on GKE)
            - name: GCP_PROJECT_ID
              value: "PROJECT_ID"

          resources:
            requests:
              cpu: 500m
              memory: 512Mi
            limits:
              cpu: 2000m
              memory: 2Gi

          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 2

          startupProbe:
            httpGet:
              path: /startup
              port: http
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 12

        # OpenTelemetry Collector sidecar
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:0.91.0
          imagePullPolicy: IfNotPresent

          args:
            - "--config=/etc/otel/config.yaml"

          ports:
            - name: otlp-grpc
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
            - name: metrics
              containerPort: 8888
              protocol: TCP
            - name: health
              containerPort: 13133
              protocol: TCP

          volumeMounts:
            - name: otel-collector-config
              mountPath: /etc/otel
              readOnly: true

          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 500m
              memory: 512Mi

          livenessProbe:
            httpGet:
              path: /
              port: health
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5

          readinessProbe:
            httpGet:
              path: /
              port: health
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3

      volumes:
        - name: otel-collector-config
          configMap:
            name: otel-collector-config

      # Security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000

      # Node affinity for optimal placement (optional)
      # affinity:
      #   nodeAffinity:
      #     preferredDuringSchedulingIgnoredDuringExecution:
      #       - weight: 100
      #         preference:
      #           matchExpressions:
      #             - key: cloud.google.com/gke-nodepool
      #               operator: In
      #               values:
      #                 - default-pool

---
# Service
apiVersion: v1
kind: Service
metadata:
  name: data-bridge-api
  namespace: data-bridge
  labels:
    app: data-bridge
    component: api
  annotations:
    cloud.google.com/neg: '{"ingress": true}'
spec:
  type: ClusterIP
  selector:
    app: data-bridge
    component: api
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP

---
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: data-bridge-api
  namespace: data-bridge
  labels:
    app: data-bridge
    component: api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: data-bridge-api

  minReplicas: 3
  maxReplicas: 20

  metrics:
    # Scale based on CPU utilization
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # Scale based on memory utilization
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 2
          periodSeconds: 30
      selectPolicy: Max

---
# PodDisruptionBudget for high availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: data-bridge-api
  namespace: data-bridge
  labels:
    app: data-bridge
    component: api
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: data-bridge
      component: api

---
# Secret (template - replace with actual values)
# apiVersion: v1
# kind: Secret
# metadata:
#   name: data-bridge-secrets
#   namespace: data-bridge
# type: Opaque
# stringData:
#   mongodb-uri: "mongodb://username:password@mongodb:27017/database"

---
# Instructions for Workload Identity setup:
#
# 1. Create GCP service account:
#    gcloud iam service-accounts create data-bridge \
#      --project=PROJECT_ID
#
# 2. Grant Cloud Trace Agent role:
#    gcloud projects add-iam-policy-binding PROJECT_ID \
#      --member="serviceAccount:data-bridge@PROJECT_ID.iam.gserviceaccount.com" \
#      --role="roles/cloudtrace.agent"
#
# 3. Bind Kubernetes SA to GCP SA:
#    gcloud iam service-accounts add-iam-policy-binding \
#      data-bridge@PROJECT_ID.iam.gserviceaccount.com \
#      --role=roles/iam.workloadIdentityUser \
#      --member="serviceAccount:PROJECT_ID.svc.id.goog[data-bridge/data-bridge-app]"
#
# 4. Apply manifests:
#    kubectl apply -f k8s-manifests.yaml
#
# 5. Verify traces in Cloud Trace:
#    https://console.cloud.google.com/traces
