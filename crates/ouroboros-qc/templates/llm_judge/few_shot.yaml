# Few-Shot LLM Judge Template
name: llm_judge_few_shot
version: "1.0.0"
description: "LLM-as-judge with few-shot examples for improved consistency"

system_role: "You are an expert evaluator assessing the quality of an AI agent's response. Use the examples below to calibrate your evaluation."

# Few-shot examples
examples:
  - input: "What is 2+2?"
    output: |
      {
        "scores": {
          "accuracy": 1.0,
          "relevance": 1.0,
          "clarity": 1.0
        },
        "feedback": "Perfect answer: correct, relevant, and clear"
      }
    explanation: "A correct mathematical answer deserves full scores"

  - input: "What is the capital of France?"
    output: |
      {
        "scores": {
          "accuracy": 1.0,
          "relevance": 1.0,
          "clarity": 0.9
        },
        "feedback": "Correct answer but could be more concise"
      }
    explanation: "Slightly verbose but accurate response"

  - input: "Tell me about the weather"
    output: |
      {
        "scores": {
          "accuracy": 0.0,
          "relevance": 0.5,
          "clarity": 0.8
        },
        "feedback": "No weather data provided, only generic response"
      }
    explanation: "Agent failed to provide actual weather information"

sections:
  - title: "Input"
    content: "{{input}}"

  - title: "Expected Output"
    content: "{{expected}}"
    optional: true
    condition: "has_expected"

  - title: "Actual Output"
    content: "{{actual}}"

  - title: "Evaluation Criteria"
    content: |
      {{criteria}}

  - title: "Your Task"
    content: |
      Following the examples above, evaluate the actual output against each criterion.

      Assign a score from 0.0 (terrible) to 1.0 (perfect) for each criterion.

      Respond in JSON format:
      ```json
      {
        "scores": {
          {{criteria_keys}}
        },
        "feedback": "Brief explanation of the evaluation"
      }
      ```

metadata:
  technique: "few_shot"
  temperature: "0.0"
  use_case: "consistent_evaluation"
  examples_count: "3"
