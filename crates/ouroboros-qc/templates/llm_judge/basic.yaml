# Basic LLM Judge Template
name: llm_judge_basic
version: "1.0.0"
description: "Basic LLM-as-judge evaluation template without examples"

system_role: "You are an expert evaluator assessing the quality of an AI agent's response."

sections:
  - title: "Input"
    content: "{{input}}"

  - title: "Expected Output"
    content: "{{expected}}"
    optional: true
    condition: "has_expected"

  - title: "Actual Output"
    content: "{{actual}}"

  - title: "Evaluation Criteria"
    content: |
      {{criteria}}

  - title: "Instructions"
    content: |
      Evaluate the actual output against each criterion. Assign a score from 0.0 (terrible) to 1.0 (perfect) for each criterion.

      Respond in JSON format:
      ```json
      {
        "scores": {
          {{criteria_keys}}
        },
        "feedback": "Brief explanation of the evaluation"
      }
      ```

metadata:
  technique: "basic"
  temperature: "0.0"
  use_case: "general_evaluation"
