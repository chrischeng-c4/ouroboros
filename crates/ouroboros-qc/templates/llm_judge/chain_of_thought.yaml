# Chain-of-Thought LLM Judge Template
name: llm_judge_cot
version: "1.0.0"
description: "LLM-as-judge with chain-of-thought reasoning for better explainability"

system_role: "You are an expert evaluator assessing the quality of an AI agent's response. Think step-by-step through your evaluation."

sections:
  - title: "Input"
    content: "{{input}}"

  - title: "Expected Output"
    content: "{{expected}}"
    optional: true
    condition: "has_expected"

  - title: "Actual Output"
    content: "{{actual}}"

  - title: "Evaluation Criteria"
    content: |
      {{criteria}}

  - title: "Instructions"
    content: |
      Let's evaluate step by step:

      **Step 1: Analyze each criterion separately**
      For each criterion:
      1. Identify what the criterion requires
      2. Check if the actual output meets the requirement
      3. Provide specific evidence from the output
      4. Assign a score (0.0-1.0)

      **Step 2: Consider the overall quality**
      - Are there any critical failures?
      - Is the response fundamentally correct?
      - Does it address the user's intent?

      **Step 3: Provide final scores**

      Respond in JSON format with your reasoning:
      ```json
      {
        "reasoning": {
          "step1": "Analysis for each criterion...",
          "step2": "Overall quality assessment...",
          "step3": "Final decision rationale..."
        },
        "scores": {
          {{criteria_keys}}
        },
        "feedback": "Summary of evaluation"
      }
      ```

metadata:
  technique: "chain_of_thought"
  temperature: "0.0"
  use_case: "explainable_evaluation"
  output_includes_reasoning: "true"
