# Self-Consistency LLM Judge Template
name: llm_judge_self_consistency
version: "1.0.0"
description: "LLM-as-judge optimized for multiple sampling and majority voting"

system_role: "You are an expert evaluator assessing the quality of an AI agent's response. Provide a single, decisive evaluation."

sections:
  - title: "Input"
    content: "{{input}}"

  - title: "Expected Output"
    content: "{{expected}}"
    optional: true
    condition: "has_expected"

  - title: "Actual Output"
    content: "{{actual}}"

  - title: "Evaluation Criteria"
    content: |
      {{criteria}}

  - title: "Instructions"
    content: |
      Evaluate the actual output against each criterion independently.

      Important: Be decisive in your scoring. Avoid middling scores unless truly warranted.
      - If something is mostly right, give a high score (0.8-1.0)
      - If something is mostly wrong, give a low score (0.0-0.3)
      - Only use middle scores (0.4-0.7) if truly uncertain

      Assign a score from 0.0 (terrible) to 1.0 (perfect) for each criterion.

      Respond in JSON format:
      ```json
      {
        "scores": {
          {{criteria_keys}}
        },
        "feedback": "Brief explanation",
        "confidence": "high|medium|low"
      }
      ```

metadata:
  technique: "self_consistency"
  temperature: "0.7"
  use_case: "high_reliability_evaluation"
  recommended_samples: "5"
  voting_strategy: "majority_or_average"
