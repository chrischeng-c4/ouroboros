# ouroboros-agent Testing Guide

## Quick Start / å¿«é€Ÿé–‹å§‹

### 1. Set OpenAI API Key / è¨­å®š OpenAI API Key

```bash
export OPENAI_API_KEY="sk-..."
```

### 2. Build Python Bindings / ç·¨è­¯ Python ç¶å®š

```bash
uv run --with maturin maturin develop
```

**Expected Output / é æœŸè¼¸å‡º**:
```
âœï¸  Setting installed package as editable
ğŸ›   Installed ouroboros-kit-0.1.0
```

### 3. Run Integration Tests / åŸ·è¡Œæ•´åˆæ¸¬è©¦

```bash
uv run python python/examples/agent/integration_test.py
```

---

## Test Suite Overview / æ¸¬è©¦å¥—ä»¶æ¦‚è¦½

The integration test validates all agent framework functionality:

æ•´åˆæ¸¬è©¦é©—è­‰æ‰€æœ‰ agent æ¡†æ¶åŠŸèƒ½ï¼š

### Test 1: Module Imports / æ¨¡çµ„å°å…¥
- âœ… Import Agent, OpenAI, Tool, ToolRegistry, get_global_registry
- Validates Python bindings are working

### Test 2: OpenAI Provider / OpenAI æä¾›è€…
- âœ… Create OpenAI provider with API key
- âœ… Check supported models
- Validates LLM integration

### Test 3: Basic Agent Execution / åŸºæœ¬ Agent åŸ·è¡Œ
- âœ… Create agent with system prompt
- âœ… Send simple query to OpenAI
- âœ… Get response with token usage
- Validates end-to-end agent execution

### Test 4: Tool Creation & Registration / å·¥å…·å»ºç«‹èˆ‡è¨»å†Š
- âœ… Create async tool
- âœ… Register in ToolRegistry
- âœ… Verify registration
- Validates tool structure

### Test 5: Tool Execution / å·¥å…·åŸ·è¡Œ
- âœ… Execute sync Python function
- âœ… Execute async Python function
- âœ… Handle complex return values
- **Validates Phase 2 critical feature** (Python function tool wrapping)

### Test 6: Advanced Queries / é€²éšæŸ¥è©¢
- âœ… Multiple agent queries
- âœ… Different models (gpt-3.5-turbo)
- âœ… Parameter variations
- Validates production usage patterns

---

## Expected Output / é æœŸè¼¸å‡º

```
ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€
  ouroboros.agent Integration Test Suite
  Testing Phase 1 (MVP) + Phase 2 (Tool Execution)
ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€ ğŸš€

â„¹ API key: sk-proj-uY...iZQA

======================================================================
  Test 1: Module Imports
======================================================================
âœ“ Imported Agent
âœ“ Imported OpenAI
âœ“ Imported Tool
âœ“ Imported ToolRegistry
âœ“ Imported get_global_registry

======================================================================
  Test 2: OpenAI Provider
======================================================================
âœ“ Created OpenAI provider: openai
âœ“ Supports 8 models
â„¹ Models: gpt-4, gpt-3.5-turbo, gpt-4-turbo, gpt-4o, gpt-4o-mini...

======================================================================
  Test 3: Basic Agent Execution
======================================================================
âœ“ Created agent: test_agent
â„¹ Agent ID: test_agent
â„¹ Max turns: 10
â„¹ Tool timeout: 30s
â„¹ Sending query: 'What is 2+2? Answer in 3 words or less.'
âœ“ Got response in 1.23s
â„¹ Content: 2 + 2 = 4
â„¹ Model: gpt-4
â„¹ Finish reason: stop
â„¹ Tokens: 23 (prompt: 15, completion: 8)

======================================================================
  Test 4: Tool Creation & Registration
======================================================================
âœ“ Created tool: calculate
â„¹ Description: Evaluate a mathematical expression
â„¹ Parameters: 1
âœ“ Registered tool (registry count: 1)
âœ“ Tool found in registry

======================================================================
  Test 5: Tool Execution
======================================================================
âœ“ Sync tool executed: Hello, Alice!
âœ“ Async tool executed: 56
âœ“ Complex data tool executed

======================================================================
  Test 6: Advanced Queries
======================================================================
â„¹ Query: Name a programming language
âœ“ Response (0.87s): Python
â„¹ Tokens used: 12
â„¹ Query: What is the capital of France? One word answer.
âœ“ Response (0.65s): Paris
â„¹ Tokens used: 10
â„¹ Query: Calculate 15 * 3. Just give the number.
âœ“ Response (0.71s): 45
â„¹ Tokens used: 11

======================================================================
  Test Summary
======================================================================
âœ“ PASS - Module Imports
âœ“ PASS - OpenAI Provider
âœ“ PASS - Basic Agent
âœ“ PASS - Tool Creation
âœ“ PASS - Tool Execution
âœ“ PASS - Advanced Queries

Total: 6/6 passed (100.0%)
Duration: 4.52s

ğŸ‰ All tests passed! Agent framework is working correctly.
âœ… Phase 1 (MVP): Complete
âœ… Phase 2 (Tool Execution): Complete
```

---

## Individual Examples / å€‹åˆ¥ç¯„ä¾‹

### Simple Agent Example

```bash
uv run python python/examples/agent/simple_agent.py
```

**What it does / åŠŸèƒ½**:
- Creates OpenAI provider
- Creates agent with system prompt
- Runs 3 example queries with different models/parameters
- Shows response metadata (tokens, model, finish reason)

### Tool Agent Example

```bash
uv run python python/examples/agent/tool_agent.py
```

**What it does / åŠŸèƒ½**:
- Creates 3 tools (search, weather, calculator)
- Registers tools in global registry
- **Executes tools directly** (demonstrates Phase 2 tool wrapping)
- Shows tool execution results

---

## Unit Tests / å–®å…ƒæ¸¬è©¦

Basic unit tests (no API key required):

```bash
uv run pytest python/tests/agent/test_agent_basic.py -v
```

**Tests / æ¸¬è©¦**:
- OpenAI provider creation
- Tool creation with parameters
- ToolRegistry operations (register, unregister, contains, clear)
- Agent configuration

Tool execution tests (validates Phase 2):

```bash
uv run pytest python/tests/agent/test_tool_execution.py -v
```

**Tests / æ¸¬è©¦**:
- Sync function tool execution
- Async function tool execution
- String/integer arguments
- Complex dict returns
- Error handling
- Registry integration

---

## Troubleshooting / æ•…éšœæ’é™¤

### Error: OPENAI_API_KEY not set

```bash
export OPENAI_API_KEY="sk-..."
```

### Error: Module 'ouroboros.agent' not found

Rebuild Python bindings:

```bash
uv run --with maturin maturin develop
```

### Error: maturin build failed

Check if pre-existing issues in other crates (postgres, api):

```bash
# Try building just agent crates
cargo build -p ouroboros-agent-core -p ouroboros-agent-llm -p ouroboros-agent-tools
```

Currently disabled in `pyproject.toml` due to compilation errors:
- `postgres`: ExtractedValue::Decimal type mismatch
- `api`: TypeDescriptor missing BSON patterns

### Rate Limiting / API Errors

If you get rate limit errors from OpenAI:
- Use gpt-3.5-turbo (cheaper, higher limits)
- Add delays between requests
- Check your OpenAI API quota

---

## What's Being Tested / æ¸¬è©¦å…§å®¹

### âœ… Phase 1 (MVP) - Complete

| Feature | Status | Test |
|---------|--------|------|
| OpenAI integration | âœ… | Test 2, 3 |
| Basic agent execution | âœ… | Test 3 |
| Tool structure | âœ… | Test 4 |
| Python bindings (PyO3) | âœ… | Test 1 |
| Response metadata | âœ… | Test 3, 6 |

### âœ… Phase 2 (Tool Execution) - Complete

| Feature | Status | Test |
|---------|--------|------|
| Python function wrapping | âœ… | Test 5 |
| Sync function execution | âœ… | Test 5 |
| Async function execution | âœ… | Test 5 |
| Tool registration | âœ… | Test 4 |
| Complex return values | âœ… | Test 5 |
| GIL-free execution | âœ… | Test 5 |

### âŒ Phase 2 (Remaining) - Pending

| Feature | Status | Priority |
|---------|--------|----------|
| Anthropic Claude provider | âŒ | High |
| Streaming responses | âŒ | Critical |
| Human-in-the-loop | âŒ | Critical |
| Persistent memory (MongoDB) | âŒ | High |

---

## Performance Validation / æ€§èƒ½é©—è­‰

The integration test measures:
- **Latency**: Response time per query (~1-2s for GPT-4)
- **Token usage**: Tracks prompt/completion/total tokens
- **GIL release**: Tools execute outside Python GIL (async)

Expected performance:
- **Simple queries**: 0.5-1.5s (gpt-3.5-turbo)
- **Complex queries**: 1-3s (gpt-4)
- **Tool execution**: <100ms overhead
- **Memory**: Efficient Arc-based state sharing

---

## Next Steps / ä¸‹ä¸€æ­¥

After successful testing:

1. **Add Anthropic Claude Provider** (Gap #1)
   - Support more LLM providers
   - Reduce vendor lock-in

2. **Implement Streaming** (Gap #3)
   - Real-time token streaming
   - Better UX for long responses

3. **Human-in-the-Loop** (Gap #4)
   - Tool call approval
   - Conditional approval logic

4. **Persistent Memory** (Gap #5)
   - MongoDB backend
   - Long-term conversation history

---

## API Costs / API æˆæœ¬

Estimated costs for testing (varies by model):

| Model | Cost per 1K tokens | Integration Test | All Examples |
|-------|-------------------|------------------|--------------|
| gpt-4 | ~$0.03 | ~$0.10 | ~$0.20 |
| gpt-3.5-turbo | ~$0.002 | ~$0.01 | ~$0.02 |
| gpt-4-turbo | ~$0.01 | ~$0.03 | ~$0.06 |

**Recommendation / å»ºè­°**: Use gpt-3.5-turbo for frequent testing to minimize costs.

---

## Contact / è¯çµ¡

If tests fail or you encounter issues:
1. Check this TESTING.md for troubleshooting
2. Review GAP_ANALYSIS.md for known limitations
3. Check build output for compilation errors

**Status / ç‹€æ…‹**:
- âœ… Phase 1 (MVP): Production-ready
- âœ… Phase 2 (Tool Execution): Production-ready
- ğŸ”„ Phase 2 (Remaining): In progress
